<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Shaoan Wang</title>

    <meta name="author" content="Shaoan Wang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Shaoan Wang
                </p>
                <p>I'm a fifth-year PhD student at School of Advanced Manufacturing and Robotics, Peking University, where I have been advised by Prof. <a href="https://www.coe.pku.edu.cn/teaching/manufacturing/9993.html">Junzhi Yu</a>. Currently, I'm a research intern at ByteDance Seed Robotics. Previously, I'm a research intern at an embodied AI startup <a href="https://www.galbot.com/">GALBOT</a>, supervised by Prof. <a href="https://hughw19.github.io">He Wang</a> and Dr. <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ&hl=en">Zhizheng Zhang</a>. Before that, I completed my Bachelor's in Robotics at Beijing Institute of Technology in 2021, supervised by Prof. <a href="https://smen.bit.edu.cn/szdw/szml/znjqryjs/qb08/4ada85fc3d4e4ebd8d84480267e36754.htm#">Huaping Wang</a>.
                </p>
                <p>Nowadays, I'm interested in embodied AI, with a specific focus on visual-language navigation and multimodal robot perception and planning. In my previous work, I concentrated on achieving high‑precision pose estimation for curved surfaces by leveraging visual fiducial markers.
                </p>
                <p style="text-align:center">
                  <a href="mailto:wangshaoan@stu.pku.edu.cn">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=m3FCQrkAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/wsakobe/">GitHub</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/wsa.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/wsa.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications (Selected)</h2>
                
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr onmouseout="vlingnav_stop()" onmouseover="vlingnav_start()" >
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='vlingnav_image'>
                  <img src="images/vlingnav/vlingnav.gif" width="230" height="135">
                  </div>
                  <img src='images/vlingnav/vlingnav.png' width="230" height="135">
                </div>
                <script type="text/javascript">
                  function vlingnav_start() {
                    document.getElementById('vlingnav_image').style.opacity = "1";
                  }
                  function vlingnav_stop() {
                    document.getElementById('vlingnav_image').style.opacity = "0";
                  }
                  vlingnav_stop()
                </script>
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <a href="https://wsakobe.github.io/VLingNav-web/">
                <papertitle>VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory</papertitle>
                </a>
                <br>
                Shaoan Wang*, Yuanfei Luo*, Xingyu Chen†, Aocheng Luo, Dongyue Li, Chang Liu, Sheng Chen‡, Yangang Zhang, Junzhi Yu†
                <br>
                <em><b style="font-weight: bold;">Arxiv Preprint</b></em>
                <br>
                <a href="https://arxiv.org/abs/2601.08665">Paper</a>
                /
                <!-- <a href="https://github.com/wsakobe/TrackVLA">Code</a> -->
                <!-- / -->
                <a href="https://wsakobe.github.io/VLingNav-web/">Project page</a>
                <p></p>
                <p>
                  VLingNav is a VLA model for embodied navigation that combines an adaptive chain-of-thought mechanism (AdaCoT) to trigger explicit reasoning only when needed with a visual-assisted linguistic memory module (VLingMem) to build persistent cross-modal semantic memory for long-horizon navigation.
                </p>
              </td>
            </tr>

            <tr onmouseout="navfom_stop()" onmouseover="navfom_start()" >
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='navfom_image'>
                  <img src="images/navfom/navfom.gif" width="230" height="165">
                  </div>
                  <img src='images/navfom/navfom.png' width="230" height="165">
                </div>
                <script type="text/javascript">
                  function navfom_start() {
                    document.getElementById('navfom_image').style.opacity = "1";
                  }
                  function navfom_stop() {
                    document.getElementById('navfom_image').style.opacity = "0";
                  }
                  navfom_stop()
                </script>
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <a href="https://pku-epic.github.io/NavFoM-Web/">
                <papertitle>Embodied Navigation Foundation Model</papertitle>
                </a>
                <br>
                  Jiazhao Zhang*, Anqi Li*, Yunpeng Qi*, Minghan Li*, Jiahang Liu, Shaoan Wang, Haoran Liu, Gengze Zhou, Yuze Wu, Xingxing Li, Yuxin Fan, Wenjun Li, Zhibo Chen, Fei Gao, Qi Wu, Zhizheng Zhang†, He Wang†
                <br>
                <em><b style="font-weight: bold;">Arxiv Preprint</b></em>
                <br>
                <a href="https://arxiv.org/abs/2509.12129">Paper</a>
                /
                <!-- <a href="https://github.com/wsakobe/TrackVLA">Code</a> -->
                <!-- / -->
                <a href="https://pku-epic.github.io/NavFoM-Web/">Project page</a>
                <p></p>
                <p>
                  We introduce a cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained on eight million navigation samples that encompass quadrupeds, drones, wheeled robots, and vehicles, and spanning diverse tasks such as vision-and-language navigation, object searching, target tracking, and autonomous driving.
                </p>
              </td>
            </tr>
  
            <tr onmouseout="trackvla_plus_plus_stop()" onmouseover="trackvla_plus_plus_start()" >
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='trackvla_plus_plus_image'>
                  <img src="images/trackvla++/trackvla++.gif" width="230" height="165">
                  </div>
                  <img src='images/trackvla++/trackvla++.png' width="230" height="165">
                </div>
                <script type="text/javascript">
                  function trackvla_plus_plus_start() {
                    document.getElementById('trackvla_plus_plus_image').style.opacity = "1";
                  }
                  function trackvla_plus_plus_stop() {
                    document.getElementById('trackvla_plus_plus_image').style.opacity = "0";
                  }
                  trackvla_plus_plus_stop()
                </script>
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <a href="https://pku-epic.github.io/TrackVLA-plus-plus-Web/">
                <papertitle>TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking</papertitle>
                </a>
                <br>
                  Jiahang Liu*, Yunpeng Qi*, Jiazhao Zhang*, Minghan Li, Shaoan Wang, Kui Wu, Hanjing Ye, Hong Zhang, Zhibo Chen, Fangwei Zhong, Zhizheng Zhang†, He Wang†
                <br>
                <em><b style="font-weight: bold;">Arxiv Preprint</b></em>
                <br>
                <a href="https://arxiv.org/pdf/2510.07134">Paper</a>
                /
                <!-- <a href="https://github.com/wsakobe/TrackVLA">Code</a> -->
                <!-- / -->
                <a href="https://pku-epic.github.io/TrackVLA-plus-plus-Web/">Project page</a>
                <p></p>
                <p>
                  TrackVLA++ is a novel Vision-Language-Action model that incorporates spatial reasoning and target identification memory, enabling superior performance in both long-horizon and highly crowded tracking scenarios.
                </p>
              </td>
            </tr>

            <tr onmouseout="trackvla_stop()" onmouseover="trackvla_start()" >
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='trackvla_image'>
                  <img src="images/trackvla/trackvla.gif" width="220" height="165">
                  </div>
                  <img src='images/trackvla/trackvla.png' width="220" height="165">
                </div>
                <script type="text/javascript">
                  function trackvla_start() {
                    document.getElementById('trackvla_image').style.opacity = "1";
                  }
                  function trackvla_stop() {
                    document.getElementById('trackvla_image').style.opacity = "0";
                  }
                  trackvla_stop()
                </script>
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <a href="https://pku-epic.github.io/TrackVLA-web/">
                <span class="papertitle">TrackVLA: Embodied Visual Tracking in the Wild</span>
                </a>
                <br>
                <strong>Shaoan Wang</strong>*, Jiazhao Zhang*, Minghan Li, Jiahang Liu, Anqi Li, Kui Wu, Fangwei Zhong, Junzhi Yu, Zhizheng Zhang<sup>†</sup>, He Wang<sup>†</sup> 
                <br>
                <em><b>CoRL 2025</b></em>
                <br>
                <a href="https://arxiv.org/pdf/2505.23189">[PDF]</a>
                <a href="https://pku-epic.github.io/TrackVLA-web/">[Project page]</a>
                <a href="https://github.com/wsakobe/TrackVLA">[Code & Benchmark]</a>
                <p></p>
                <p>
                  We present TrackVLA, a vision-language-action model capable of simultaneous object recognition and visual tracking, enabling zero-shot deployment in real-world scenarios.
                </p>
              </td>
            </tr>

            <tr onmouseout="uni_navid_stop()" onmouseover="uni_navid_start()" >
              <td style="padding:20px;width:30%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='uni_navid_image'>
                  <img src="images/uni-navid/uni_navid.gif" width="220" height="165">
                  </div>
                  <img src='images/uni-navid/uni_navid.png' width="220" height="165">
                </div>
                <script type="text/javascript">
                  function uni_navid_start() {
                    document.getElementById('uni_navid_image').style.opacity = "1";
                  }
                  function uni_navid_stop() {
                    document.getElementById('uni_navid_image').style.opacity = "0";
                  }
                  uni_navid_stop()
                </script>
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <a href="https://pku-epic.github.io/Uni-NaVid/">
                <span class="papertitle">Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks</span>
                </a>
                <br>
                Jiazhao Zhang, Kunyu Wang, <strong>Shaoan Wang(Core contributor)</strong>, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang<sup>†</sup>, He Wang<sup>†</sup> 
                <br>
                <em><b>RSS 2025</b></em>
                <br>
                <a href="https://arxiv.org/pdf/2412.06224">[PDF]</a>
                <a href="https://pku-epic.github.io/Uni-NaVid/">[Project page]</a>
                <a href="https://github.com/jzhzhang/Uni-NaVid">[Code]</a>
                <p></p>
                <p>
                  We present Uni-NaVid, the first video-based vision-language-action (VLA) model designed to unify diverse embodied navigation tasks and enable seamless navigation for mixed long-horizon tasks in unseen real-world environments.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/ef-calib.png' width="220">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">EF-Calib: Spatiotemporal Calibration of Event- and Frame-based Cameras Using Continuous-Time Trajectories</span>
                <br>
                <strong>Shaoan Wang</strong>, Zhanhua Xin, Yaoqing Hu, Dongyue Li, Mingzhu Zhu, Junzhi Yu<sup>†</sup></a>
                <br>
                <em><b>IEEE Robotics and Automation Letters</b></em>, 2024
                <br>
                <a href="data/2024ral.pdf">[PDF]</a>
                <a href="data/ral2024.bib">[BibTeX]</a>
                <a href="https://github.com/wsakobe/EF-Calib">[Code]</a>
                <p></p>
                <p>A novel spatiotemporal calibration toolkit for event‑based and frame-based camera system.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/suturerobot.png' width="220">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Spatially Compact Visual Navigation System for Automated Suturing Robot Toward Oral and Maxillofacial Surgery</span>
                <br>
                <strong>Shaoan Wang</strong>, Qiming Zhao, Dongyue Li, Yaoqing Hu, Mingzhu Zhu, Fusong Yuan, Jinyan Shao, Junzhi Yu<sup>†</sup></a>
                <br>
                <em><b>IEEE Transactions on Instrumentation and Measurement</b></em>, 2024
                <br>
                <a href="data/2024tim.pdf">[PDF]</a>
                <a href="data/tim2024.bib">[BibTeX]</a>
                
                <p></p>
                <p>A spatially compact visual navigation system for autonomous OMS (Oral and Maxillofacial Surgery) suture robot.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/cylindertag.png' width="220">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
             
                  <span class="papertitle">CylinderTag: An Accurate and Flexible Marker for Cylinder-Shape Objects Pose Estimation Based on Projective Invariants</span>
                
                <br>
                <strong>Shaoan Wang</strong>, Mingzhu Zhu, Yaoqing Hu, Dongyue Li, Fusong Yuan, Junzhi Yu<sup>†</sup></a>
                <br>
                <em><b>IEEE Transactions on Visualization and Computer Graphics</b></em>, 2024
                <br>
                <a href="data/2024tvcg.pdf">[PDF]</a>
                <a href="data/tvcg2024.bib">[BibTeX]</a>
                <a href="https://github.com/wsakobe/CylinderTag">[Code]</a>
                <p></p>
                <p>A visual marker based on projective invariance called CylinderTag is developed, which provides a new solution for high‑precision position estimation of curved objects.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/tim22.png' width="220">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                
                  <span class="papertitle">Accurate Detection and Localization of Curved Checkerboard-Like Marker Based on Quadratic Form</span>
                
                <br>
                <strong>Shaoan Wang</strong>, Mingzhu Zhu, Yaoqing Hu, Dongyue Li, Fusong Yuan, Junzhi Yu*</a>
                <br>
                <em><b>IEEE Transactions on Instrumentation and Measurement</b></em>, 2022
                <br>
                <a href="data/2022tim.pdf">[PDF]</a>
                <a href="data/tim2022.bib">[BibTeX]</a>
                <a href="https://github.com/wsakobe/Marker_curved-surface">[Code]</a>
                <p></p>
                <p>A subpixel corner detector for curved checkerboard-like markers based on quadratic form.</p>
              </td>
            </tr>

          </tbody></table>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  The website template was adapted from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>